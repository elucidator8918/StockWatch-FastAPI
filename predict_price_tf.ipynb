{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stock_data(ticker):\n",
    "    stock = yf.Ticker(ticker)\n",
    "    stock_prices = stock.history(period='5y', interval='1d')\n",
    "    return stock_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-11-14 00:00:00+05:30</th>\n",
       "      <td>619.580269</td>\n",
       "      <td>631.547473</td>\n",
       "      <td>619.580269</td>\n",
       "      <td>627.543579</td>\n",
       "      <td>10539902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-15 00:00:00+05:30</th>\n",
       "      <td>630.880113</td>\n",
       "      <td>634.350176</td>\n",
       "      <td>624.874273</td>\n",
       "      <td>626.297913</td>\n",
       "      <td>7362623</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-18 00:00:00+05:30</th>\n",
       "      <td>627.276567</td>\n",
       "      <td>631.725337</td>\n",
       "      <td>622.204959</td>\n",
       "      <td>627.454529</td>\n",
       "      <td>7490916</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-19 00:00:00+05:30</th>\n",
       "      <td>631.636438</td>\n",
       "      <td>635.284408</td>\n",
       "      <td>627.276622</td>\n",
       "      <td>634.261169</td>\n",
       "      <td>6495885</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-20 00:00:00+05:30</th>\n",
       "      <td>634.216814</td>\n",
       "      <td>636.174284</td>\n",
       "      <td>627.276741</td>\n",
       "      <td>634.394775</td>\n",
       "      <td>6142171</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-08 00:00:00+05:30</th>\n",
       "      <td>1818.000000</td>\n",
       "      <td>1840.599976</td>\n",
       "      <td>1813.150024</td>\n",
       "      <td>1829.949951</td>\n",
       "      <td>4210960</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-11 00:00:00+05:30</th>\n",
       "      <td>1829.000000</td>\n",
       "      <td>1868.000000</td>\n",
       "      <td>1822.550049</td>\n",
       "      <td>1860.099976</td>\n",
       "      <td>3804234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-12 00:00:00+05:30</th>\n",
       "      <td>1871.099976</td>\n",
       "      <td>1881.000000</td>\n",
       "      <td>1861.000000</td>\n",
       "      <td>1868.800049</td>\n",
       "      <td>5012450</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-13 00:00:00+05:30</th>\n",
       "      <td>1861.099976</td>\n",
       "      <td>1873.199951</td>\n",
       "      <td>1856.300049</td>\n",
       "      <td>1868.400024</td>\n",
       "      <td>4257495</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-14 00:00:00+05:30</th>\n",
       "      <td>1865.599976</td>\n",
       "      <td>1876.000000</td>\n",
       "      <td>1860.150024</td>\n",
       "      <td>1873.550049</td>\n",
       "      <td>714287</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1241 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Open         High          Low        Close  \\\n",
       "Date                                                                            \n",
       "2019-11-14 00:00:00+05:30   619.580269   631.547473   619.580269   627.543579   \n",
       "2019-11-15 00:00:00+05:30   630.880113   634.350176   624.874273   626.297913   \n",
       "2019-11-18 00:00:00+05:30   627.276567   631.725337   622.204959   627.454529   \n",
       "2019-11-19 00:00:00+05:30   631.636438   635.284408   627.276622   634.261169   \n",
       "2019-11-20 00:00:00+05:30   634.216814   636.174284   627.276741   634.394775   \n",
       "...                                ...          ...          ...          ...   \n",
       "2024-11-08 00:00:00+05:30  1818.000000  1840.599976  1813.150024  1829.949951   \n",
       "2024-11-11 00:00:00+05:30  1829.000000  1868.000000  1822.550049  1860.099976   \n",
       "2024-11-12 00:00:00+05:30  1871.099976  1881.000000  1861.000000  1868.800049   \n",
       "2024-11-13 00:00:00+05:30  1861.099976  1873.199951  1856.300049  1868.400024   \n",
       "2024-11-14 00:00:00+05:30  1865.599976  1876.000000  1860.150024  1873.550049   \n",
       "\n",
       "                             Volume  Dividends  Stock Splits  \n",
       "Date                                                          \n",
       "2019-11-14 00:00:00+05:30  10539902        0.0           0.0  \n",
       "2019-11-15 00:00:00+05:30   7362623        0.0           0.0  \n",
       "2019-11-18 00:00:00+05:30   7490916        0.0           0.0  \n",
       "2019-11-19 00:00:00+05:30   6495885        0.0           0.0  \n",
       "2019-11-20 00:00:00+05:30   6142171        0.0           0.0  \n",
       "...                             ...        ...           ...  \n",
       "2024-11-08 00:00:00+05:30   4210960        0.0           0.0  \n",
       "2024-11-11 00:00:00+05:30   3804234        0.0           0.0  \n",
       "2024-11-12 00:00:00+05:30   5012450        0.0           0.0  \n",
       "2024-11-13 00:00:00+05:30   4257495        0.0           0.0  \n",
       "2024-11-14 00:00:00+05:30    714287        0.0           0.0  \n",
       "\n",
       "[1241 rows x 7 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = stock_data('INFY.NS')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'Stock Splits'], dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-14 10:01:52.000628: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2 Max\n",
      "2024-11-14 10:01:52.000660: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 64.00 GB\n",
      "2024-11-14 10:01:52.000665: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 24.00 GB\n",
      "2024-11-14 10:01:52.000732: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-14 10:01:52.000775: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-14 10:01:57.975548: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-11-14 10:01:58.310946: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Adam/AssignAddVariableOp.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 14s 177ms/step - loss: 1.8178 - accuracy: 0.5159 - val_loss: 0.7120 - val_accuracy: 0.4815 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "24/24 [==============================] - 1s 46ms/step - loss: 1.6033 - accuracy: 0.5026 - val_loss: 0.7918 - val_accuracy: 0.4815 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "24/24 [==============================] - 1s 43ms/step - loss: 1.5397 - accuracy: 0.5212 - val_loss: 0.6988 - val_accuracy: 0.4762 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 1.5297 - accuracy: 0.5026 - val_loss: 0.7256 - val_accuracy: 0.4815 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 1.4687 - accuracy: 0.5198 - val_loss: 0.7210 - val_accuracy: 0.4815 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 1.4632 - accuracy: 0.5198 - val_loss: 0.6959 - val_accuracy: 0.5185 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 1.4632 - accuracy: 0.5172 - val_loss: 0.6956 - val_accuracy: 0.5185 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 1.4312 - accuracy: 0.5344 - val_loss: 0.6958 - val_accuracy: 0.5291 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 1.4030 - accuracy: 0.5463 - val_loss: 0.7021 - val_accuracy: 0.4974 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 1.4435 - accuracy: 0.5238 - val_loss: 0.6949 - val_accuracy: 0.5397 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 1.4135 - accuracy: 0.5569 - val_loss: 0.7087 - val_accuracy: 0.4656 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 1.4213 - accuracy: 0.5410 - val_loss: 0.7017 - val_accuracy: 0.4921 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "24/24 [==============================] - 1s 40ms/step - loss: 1.4073 - accuracy: 0.5542 - val_loss: 0.6969 - val_accuracy: 0.5079 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 1.4144 - accuracy: 0.5450 - val_loss: 0.7158 - val_accuracy: 0.4656 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "23/24 [===========================>..] - ETA: 0s - loss: 1.3744 - accuracy: 0.5462\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 1.3735 - accuracy: 0.5437 - val_loss: 0.7305 - val_accuracy: 0.4815 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 1.4140 - accuracy: 0.5463 - val_loss: 0.7184 - val_accuracy: 0.4709 - lr: 2.0000e-04\n",
      "Epoch 17/100\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 1.3850 - accuracy: 0.5661 - val_loss: 0.7126 - val_accuracy: 0.4709 - lr: 2.0000e-04\n",
      "Epoch 18/100\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 1.3708 - accuracy: 0.5741 - val_loss: 0.7107 - val_accuracy: 0.4709 - lr: 2.0000e-04\n",
      "Epoch 19/100\n",
      "24/24 [==============================] - 1s 41ms/step - loss: 1.3656 - accuracy: 0.5635 - val_loss: 0.7156 - val_accuracy: 0.4709 - lr: 2.0000e-04\n",
      "Epoch 20/100\n",
      "24/24 [==============================] - ETA: 0s - loss: 1.3691 - accuracy: 0.5741\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "24/24 [==============================] - 1s 42ms/step - loss: 1.3691 - accuracy: 0.5741 - val_loss: 0.7151 - val_accuracy: 0.4762 - lr: 2.0000e-04\n",
      "8/8 [==============================] - 1s 150ms/step - loss: 0.7338 - accuracy: 0.5105\n",
      "Test accuracy: 0.5105\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "Predicted movement for next day: Down\n",
      "Confidence: 0.3572\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.layers import Bidirectional, Conv1D, MaxPooling1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Add, Concatenate, LeakyReLU, LayerNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "class EnhancedStockPricePredictor:\n",
    "    def __init__(self, sequence_length=10):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.model = None\n",
    "        \n",
    "    def prepare_data(self, df):\n",
    "        # Enhanced feature engineering\n",
    "        df['Returns'] = df['Close'].pct_change()\n",
    "        df['Target'] = (df['Returns'].shift(-1) > 0).astype(int)\n",
    "        \n",
    "        # Technical indicators with multiple timeframes\n",
    "        for window in [5, 10, 20, 50]:\n",
    "            df[f'MA{window}'] = df['Close'].rolling(window=window).mean()\n",
    "            df[f'Std{window}'] = df['Close'].rolling(window=window).std()\n",
    "            df[f'Vol{window}'] = df['Volume'].rolling(window=window).mean()\n",
    "        \n",
    "        # Enhanced RSI calculation\n",
    "        for timeperiod in [7, 14, 21]:\n",
    "            delta = df['Close'].diff()\n",
    "            gain = (delta.where(delta > 0, 0)).rolling(window=timeperiod).mean()\n",
    "            loss = (-delta.where(delta < 0, 0)).rolling(window=timeperiod).mean()\n",
    "            rs = gain / loss\n",
    "            df[f'RSI{timeperiod}'] = 100 - (100 / (1 + rs))\n",
    "        \n",
    "        # MACD with different parameters\n",
    "        for (fast, slow) in [(12, 26), (8, 21)]:\n",
    "            exp1 = df['Close'].ewm(span=fast, adjust=False).mean()\n",
    "            exp2 = df['Close'].ewm(span=slow, adjust=False).mean()\n",
    "            df[f'MACD_{fast}_{slow}'] = exp1 - exp2\n",
    "        \n",
    "        # Momentum indicators\n",
    "        df['MOM5'] = df['Close'].diff(5)\n",
    "        df['MOM10'] = df['Close'].diff(10)\n",
    "        \n",
    "        # Volatility indicators\n",
    "        df['ATR'] = (df['High'] - df['Low']).rolling(window=14).mean()\n",
    "        \n",
    "        # Drop any rows with NaN values\n",
    "        df = df.dropna()\n",
    "        \n",
    "        # Select all numerical columns except 'Target' and 'Returns'\n",
    "        feature_columns = df.select_dtypes(include=[np.number]).columns.difference(['Target', 'Returns'])\n",
    "        \n",
    "        # Scale the features\n",
    "        scaled_features = self.scaler.fit_transform(df[feature_columns])\n",
    "        \n",
    "        # Create sequences with overlap\n",
    "        X, y = [], []\n",
    "        for i in range(len(df) - self.sequence_length):\n",
    "            X.append(scaled_features[i:(i + self.sequence_length)])\n",
    "            y.append(df['Target'].iloc[i + self.sequence_length])\n",
    "            \n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    def build_model(self, input_shape):\n",
    "        # Input layer\n",
    "        inputs = Input(shape=input_shape)\n",
    "        \n",
    "        # CNN Branch with same padding and no pooling\n",
    "        conv1 = Conv1D(64, 3, padding='same')(inputs)\n",
    "        conv1 = LeakyReLU()(conv1)\n",
    "        conv1 = BatchNormalization()(conv1)\n",
    "        \n",
    "        conv2 = Conv1D(128, 3, padding='same')(conv1)\n",
    "        conv2 = LeakyReLU()(conv2)\n",
    "        conv2 = BatchNormalization()(conv2)\n",
    "        \n",
    "        # Bidirectional LSTM Branch\n",
    "        lstm1 = Bidirectional(LSTM(64, return_sequences=True))(inputs)\n",
    "        lstm1 = LayerNormalization()(lstm1)\n",
    "        lstm1 = Dropout(0.3)(lstm1)\n",
    "        \n",
    "        lstm2 = Bidirectional(LSTM(64, return_sequences=True))(lstm1)\n",
    "        lstm2 = LayerNormalization()(lstm2)\n",
    "        lstm2 = Dropout(0.3)(lstm2)\n",
    "        \n",
    "        # Merge CNN and LSTM branches (now they have matching sequence lengths)\n",
    "        merged = Concatenate()([conv2, lstm2])\n",
    "        \n",
    "        # Additional processing\n",
    "        x = Bidirectional(LSTM(128, return_sequences=False))(merged)\n",
    "        x = LayerNormalization()(x)\n",
    "        x = Dropout(0.4)(x)\n",
    "        \n",
    "        # Dense layers with skip connections\n",
    "        dense1 = Dense(128)(x)\n",
    "        dense1 = LeakyReLU()(dense1)\n",
    "        dense1 = BatchNormalization()(dense1)\n",
    "        dense1 = Dropout(0.4)(dense1)\n",
    "        \n",
    "        dense2 = Dense(64)(dense1)\n",
    "        dense2 = LeakyReLU()(dense2)\n",
    "        dense2 = BatchNormalization()(dense2)\n",
    "        dense2 = Dropout(0.3)(dense2)\n",
    "        \n",
    "        # Output layer\n",
    "        outputs = Dense(1, activation='sigmoid')(dense2)\n",
    "        \n",
    "        # Create model\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        \n",
    "        # Compile with custom Adam configuration\n",
    "        optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        self.model = model\n",
    "        return model\n",
    "        \n",
    "    def train(self, X, y, validation_split=0.2, epochs=100, batch_size=32):\n",
    "        # Enhanced callbacks\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            mode='min'\n",
    "        )\n",
    "        \n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.2,\n",
    "            patience=5,\n",
    "            min_lr=1e-6,\n",
    "            mode='min',\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Class weights to handle imbalanced data\n",
    "        class_weights = {\n",
    "            0: 1 / np.mean(y == 0),\n",
    "            1: 1 / np.mean(y == 1)\n",
    "        }\n",
    "        \n",
    "        history = self.model.fit(\n",
    "            X, y,\n",
    "            validation_split=validation_split,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[early_stopping, reduce_lr],\n",
    "            class_weight=class_weights,\n",
    "            shuffle=True,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Usage example:\n",
    "def main():\n",
    "    # Load your stock data\n",
    "    df = stock_data('INFY.NS')\n",
    "    \n",
    "    # Initialize and prepare the model\n",
    "    predictor = EnhancedStockPricePredictor(sequence_length=10)\n",
    "    X, y = predictor.prepare_data(df)\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Build and train the model\n",
    "    predictor.build_model(input_shape=(X.shape[1], X.shape[2]))\n",
    "    history = predictor.train(X_train, y_train)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    test_loss, test_accuracy = predictor.model.evaluate(X_test, y_test)\n",
    "    print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # Make predictions for the next day\n",
    "    last_sequence = X_test[-1:]\n",
    "    prediction = predictor.predict(last_sequence)\n",
    "    print(f\"Predicted movement for next day: {'Up' if prediction[0][0] > 0.5 else 'Down'}\")\n",
    "    print(f\"Confidence: {prediction[0][0]:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
